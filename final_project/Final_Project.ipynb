{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS591 Final Project - Corey Clemente & Renzo Callejas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Functions for Scraping Twitter for Historical Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import timedelta\n",
    "import sqlite3\n",
    "import got # 3rd party library for scraping twitter\n",
    "import os\n",
    "\n",
    "def check_folder(fname): # make proper folder structure for data\n",
    "    if not os.path.isdir(fname):\n",
    "        os.mkdir(fname)\n",
    "        \n",
    "def make_new(name): # make a database file to store tweets\n",
    "    name = name.replace(' ', '_')\n",
    "    check_folder('tweet_data')\n",
    "    sqlite_file = 'tweet_data/' + name + '.db'\n",
    "    table_name = name\n",
    "    col1 = 'username'\n",
    "    col2 = 'tweet'\n",
    "    col3 = 'date'\n",
    "    col4 = 'retweets'\n",
    "    col5 = 'favorites'\n",
    "    col_type = 'TEXT'\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    c = conn.cursor()\n",
    "    c.execute('CREATE TABLE {tn} ({nf} {ft})'\\\n",
    "            .format(tn=table_name, nf=col1, ft=col_type))\n",
    "    c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\"\\\n",
    "            .format(tn=table_name, cn=col2, ct=col_type))\n",
    "    c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\"\\\n",
    "            .format(tn=table_name, cn=col3, ct=col_type))\n",
    "    c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\"\\\n",
    "            .format(tn=table_name, cn=col4, ct=col_type))\n",
    "    c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\"\\\n",
    "            .format(tn=table_name, cn=col5, ct=col_type))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_data(query, start, end, N, show=True):\n",
    "    start = parse(start)\n",
    "    end = parse(end)\n",
    "    diff = (end - start).days\n",
    "    days = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") \\\n",
    "            for i in range(diff+2)] # get list of each day to get N tweets per day\n",
    "    \n",
    "    tweet_data = []\n",
    "    try:\n",
    "        for i in range(len(days)-1):\n",
    "            if show:\n",
    "                print 'Query: %s\\n%d tweet(s) from %s\\n' % (query, N, days[i])\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query).\\\n",
    "                            setSince(days[i]).setUntil(days[i+1]).\\\n",
    "                            setMaxTweets(N) # scrape specifications\n",
    "            tweets = got.manager.TweetManager.getTweets(tweetCriteria) # actually scrape for tweets\n",
    "            if len(tweets) == 0: # IP blocked or no tweets found\n",
    "                return tweet_data\n",
    "            for tweet in tweets:\n",
    "                tweet_data += [[tweet.username,\n",
    "                                tweet.text,\n",
    "                                tweet.date.strftime(\"%Y-%m-%d\"),\n",
    "                                str(tweet.retweets),\n",
    "                                str(tweet.favorites)]]               \n",
    "        return tweet_data\n",
    "    except:\n",
    "        return tweet_data\n",
    "\n",
    "def run(N, keyword, start_date, end_date, show=True):\n",
    "    make_new(keyword) # make new database for these tweets\n",
    "    query_string = 'INSERT INTO ' + keyword.replace(' ', '_') + \\\n",
    "                   ' VALUES (?, ?, ?, ?, ?)'\n",
    "    conn = sqlite3.connect('tweet_data/' + keyword.replace(' ', '_') + '.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    to_insert = get_data(keyword, start_date, end_date, N, show=show) # all tweets\n",
    "    if to_insert != []:\n",
    "        c.executemany(query_string, to_insert) # insert all into database\n",
    "        print keyword\n",
    "        print 'Completed successfully!'\n",
    "        print 'Saving and shutting down...'\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print 'Everything saved.\\n'\n",
    "    else:\n",
    "        print keyword\n",
    "        print 'Returned empty tweet list: IP may be blocked or no tweets found!'\n",
    "        print 'Shutting down, nothing saved...'\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Functions for Parsing Tweets for Sentiment and Pairing Data with Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # sentiment analysis\n",
    "from datetime import timedelta, date\n",
    "from dateutil.parser import parse\n",
    "from yahoo_finance import Share # library to get stock data\n",
    "from textblob import TextBlob # for polarity and subjectivity analysis\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def check_folder(fname): # make proper folder structure for data\n",
    "    if not os.path.isdir(fname):\n",
    "        os.mkdir(fname)\n",
    "        \n",
    "def load_database(name): # return database as list of tuples (inside tuple is entry)\n",
    "    name = name.replace(' ', '_')\n",
    "    check_folder('tweet_data')\n",
    "    conn = sqlite3.connect('tweet_data/' + name + '.db')\n",
    "    tweets = []\n",
    "    with conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"SELECT * FROM \" + name)\n",
    "        for row in c.fetchall():\n",
    "            tweets += [row]\n",
    "        return tweets\n",
    "        \n",
    "def stock_data(stock, start_date, end_date):\n",
    "    '''\n",
    "    stock: string of stock symbol (e.g. YHOO)\n",
    "    start_date: string of min date to get data for, format is yyyy-mm-dd\n",
    "    end_date: same format as start_date, max date to get data for\n",
    "    '''\n",
    "    # get historical stock data and make chronoloogical order\n",
    "    raw_data = Share(stock).get_historical(start_date, end_date)\n",
    "    raw_data.reverse()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(raw_data)):\n",
    "        date = parse(raw_data[i]['Date']) # parse to get weekday later\n",
    "        data += [{'date': raw_data[i]['Date'], # date as string\n",
    "                  'open': float(raw_data[i]['Open']),\n",
    "                  'close': float(raw_data[i]['Close']),\n",
    "                  'weekday': date.weekday()}]  # weekday as int, Mon = 0, Sun = 6\n",
    "        data[-1]['change'] = data[-1]['close'] - data[-1]['open']\n",
    "\n",
    "    pos, change, date = [], [], [] # datasets for models later\n",
    "    for x in data:\n",
    "        if x['change'] > 0:\n",
    "            pos += [1]\n",
    "            change += [x['change']]\n",
    "            date += [(parse(x['date'])-timedelta(days=1)).strftime(\"%Y-%m-%d\")]\n",
    "        else:\n",
    "            pos += [-1]\n",
    "            change += [x['change']]\n",
    "            date += [(parse(x['date'])-timedelta(days=1)).strftime(\"%Y-%m-%d\")]\n",
    "            \n",
    "    return np.array(pos), np.array(change), np.array(date)\n",
    "    \n",
    "def build_company(stock_name, keyword):\n",
    "    pos1, change, dates = stock_data(stock_name, '2015-01-01', '2015-12-31')\n",
    "    raw_data = load_database(keyword)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    data = {}\n",
    "    for tweet in raw_data: # tweet = (user, tweet, date, retweets, favorites)\n",
    "        user, text, tdate, ret, fav = tweet\n",
    "        s = sid.polarity_scores(text)\n",
    "        com, neg, neu, pos = s['compound'], s['neg'], s['neu'], s['pos']\n",
    "        pol, sub = TextBlob(text).sentiment\n",
    "        alpha = int(ret) + int(fav) # place more weight on popular tweets\n",
    "        if tdate not in data:\n",
    "            data[tdate] = [0]*9 # [ret, favs, com, neg, neu, pos, pol, sub, count]\n",
    "        data[tdate][0] += int(ret) * alpha\n",
    "        data[tdate][1] += int(fav) * alpha\n",
    "        data[tdate][2] += com * alpha\n",
    "        data[tdate][3] += neg * alpha\n",
    "        data[tdate][4] += neu * alpha\n",
    "        data[tdate][5] += pos * alpha\n",
    "        data[tdate][6] += pol * alpha\n",
    "        data[tdate][7] += sub * alpha\n",
    "        data[tdate][8] += alpha\n",
    "\n",
    "    X = [] # input data matrix for models\n",
    "    for i in range(len(dates)): # average all featured\n",
    "        try:\n",
    "            ret = data[dates[i]][0] / float(data[dates[i]][8])\n",
    "            fav = data[dates[i]][1] / float(data[dates[i]][8])\n",
    "            com = data[dates[i]][2] / float(data[dates[i]][8])\n",
    "            neg = data[dates[i]][3] / float(data[dates[i]][8])\n",
    "            neu = data[dates[i]][4] / float(data[dates[i]][8])\n",
    "            pos = data[dates[i]][5] / float(data[dates[i]][8])\n",
    "            pol = data[dates[i]][6] / float(data[dates[i]][8])\n",
    "            sub = data[dates[i]][7] / float(data[dates[i]][8])\n",
    "            X += [[ret, fav, com, neg, neu, pos, pol, sub, change[i-1]]] # vector in training matrix\n",
    "        except:\n",
    "            X += [[ret, fav, com, neg, neu, pos, pol, sub, 0]] # first day has no past stock data\n",
    "    \n",
    "    return np.array(X), pos1, change\n",
    "    \n",
    "def make_entire_dataset(keywords, stock_names, dataset_name): # save everything to pickled file\n",
    "    data = {}\n",
    "    for i in range(len(keywords)):\n",
    "        print 'Parsing %s Tweet Data' % stock_names[i]\n",
    "        X, y1, y2 = build_company(stock_names[i], keywords[i])\n",
    "        data[stock_names[i]] = [X, y1, y2]\n",
    "    check_folder('datasets')\n",
    "    pickle.dump(data, open('datasets/' + dataset_name + '.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Functions for Loading Dataset and Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def check_folder(fname): # make proper folder structure for data\n",
    "    if not os.path.isdir(fname):\n",
    "        os.mkdir(fname)\n",
    "        \n",
    "def load_entire_dataset(dataset_name):\n",
    "    check_folder('datasets')\n",
    "    return pickle.load(open('datasets/' + dataset_name + '.p', 'rb'))\n",
    "\n",
    "def random_split(X, y, percentage): # randomly split data into training and testing\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    n = int(round(len(y)*percentage))\n",
    "    lst = list(range(len(y)))\n",
    "    np.random.shuffle(lst)\n",
    "    test = lst[:n]\n",
    "    train = lst[n:]\n",
    "    return X[train], X[test], y[train], y[test]\n",
    "        \n",
    "def predict_using_all(clfs, X_test, y_test): # random forest of all models' answers\n",
    "    X1, X2, y1, y2 = random_split(X_test, y_test, .7)\n",
    "    results = np.array([clfs[i][0].predict(X1) for i in range(len(clfs))]).T\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(results, y1)\n",
    "    results = np.array([clfs[i][0].predict(X2) for i in range(len(clfs))]).T\n",
    "    return clf.score(results, y2)\n",
    "\n",
    "def all_classifiers(dataset_name, percentage): # train each model on each company\n",
    "    data = load_entire_dataset(dataset_name)\n",
    "    best_x = [i for i in range(1, len(data) + 1)]\n",
    "    best_x_name = []\n",
    "    best_y = []\n",
    "    for company in data:\n",
    "        best = 0\n",
    "        X, y = data[company][0], data[company][1]\n",
    "        X_train, X_test, y_train, y_test = random_split(X, y, percentage)\n",
    "        classifiers = [[KNeighborsClassifier(3),'KNN'], # use 9 different classifiers\n",
    "                       [SVC(kernel=\"linear\", C=0.025),'L-SVM'],\n",
    "                       [SVC(gamma=2, C=1),'R-SVM'],\n",
    "                       [AdaBoostClassifier(), 'ABC'],\n",
    "                       [DecisionTreeClassifier(max_depth=8),'DT'],\n",
    "                       [RandomForestClassifier(max_depth=8),'RF'],\n",
    "                       [GaussianNB(),'GNB'],\n",
    "                       [LinearDiscriminantAnalysis(),'LDA'],\n",
    "                       [QuadraticDiscriminantAnalysis(),'QDA']]\n",
    "        for clf in classifiers:\n",
    "            clf[0].fit(X_train, y_train)\n",
    "            best += clf[0].score(X_test, y_test) # get accuracy\n",
    "        best_x_name += [company]\n",
    "        best_y += [best / float(9)]\n",
    "    plt.bar(best_x, best_y)\n",
    "    for i in range(len(data)):\n",
    "        plt.text(best_x[i] + .15, best_y[i] + .01, '%.2f' % best_y[i])\n",
    "    plt.xticks([x + .5 for x in best_x], best_x_name)\n",
    "    plt.title('Stock Movement Prediction Accuracy')\n",
    "    plt.xlabel('Stock')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(.4, .6) # if you know the ranges of accuracies, makes graph prettier\n",
    "    plt.show()\n",
    "    \n",
    "def aggregated(dataset_name, percentage): # \"mini\" stock market example, all tweets predict all stocks\n",
    "    data = load_entire_dataset(dataset_name)\n",
    "    best_x = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    best_x_name = []\n",
    "    best = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for company in data:\n",
    "        X_tmp, y_tmp = data[company][0], data[company][1]\n",
    "        for x in X_tmp:\n",
    "            X += [x]\n",
    "        for x in y_tmp:\n",
    "            y += [x]\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_train, X_test, y_train, y_test = random_split(X, y, percentage)\n",
    "    classifiers = [[KNeighborsClassifier(3),'KNN'],\n",
    "                   [SVC(kernel=\"linear\", C=0.025),'L-SVM'],\n",
    "                   [SVC(gamma=2, C=1),'R-SVM'],\n",
    "                   [AdaBoostClassifier(), 'ABC'],\n",
    "                   [DecisionTreeClassifier(max_depth=8),'DT'],\n",
    "                   [RandomForestClassifier(max_depth=8),'RF'],\n",
    "                   [GaussianNB(),'GNB'],\n",
    "                   [LinearDiscriminantAnalysis(),'LDA'],\n",
    "                   [QuadraticDiscriminantAnalysis(),'QDA']]\n",
    "    for clf in classifiers:\n",
    "        best_x_name += [clf[1]]\n",
    "        clf[0].fit(X_train, y_train)\n",
    "        best += [clf[0].score(X_test, y_test)]\n",
    "    plt.bar(best_x, best)\n",
    "    for i in range(9):\n",
    "        plt.text(best_x[i] + .0, best[i] + .01, '%.2f' % best[i])\n",
    "    plt.xticks([x + .5 for x in best_x], best_x_name)\n",
    "    plt.title('Stock Market Movement Prediction Accuracy')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(.4, .6) # if you know the ranges of accuracies, makes graph prettier\n",
    "    plt.show()\n",
    "    print 'Heuristic: %.3f' % predict_using_all(classifiers, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Taken for Our Exact Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# DO NOT RUN THIS - WILL TAKE ABOUT 17 HOURS\n",
    "def this_project():\n",
    "    keywords = ['citigroup', 'netflix', 'tesla', 'twitter', 'mcdonalds', 'walmart', 'microsoft', 'disney']\n",
    "    stock_names = ['C', 'NFLX', 'TSLA', 'TWTR', 'MCD', 'WMT', 'MSFT', 'DIS']\n",
    "    dataset_name = 'all_project_data'\n",
    "    tweets_per_day = 400\n",
    "    start_date = '2015-01-01'\n",
    "    end_date = '2015-12-31'\n",
    "    training_partition = .7\n",
    "\n",
    "    # part 1: get tweets and store them\n",
    "    for keyword in keywords:\n",
    "        run(tweets_per_day, keyword, start_date, end_date)\n",
    "        time.sleep(5) # to avoid IP address being blocked\n",
    "        \n",
    "    # part 2: sentiment analysis and pairing with stock data (making dataset)\n",
    "    make_entire_dataset(keywords, stock_names, dataset_name)\n",
    "    \n",
    "    # part 3: results!\n",
    "    all_classifiers(dataset_name, training_partition)\n",
    "    aggregated(dataset_name, training_partition)\n",
    "    \n",
    "# CAN RUN THIS\n",
    "def see_results_of_project():\n",
    "    dataset_name = 'all_project_data'\n",
    "    training_partition = .7\n",
    "    \n",
    "    all_classifiers(dataset_name, training_partition)\n",
    "    aggregated(dataset_name, training_partition)\n",
    "    \n",
    "see_results_of_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Your Own Small Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def make_small_version(keywords, stock_names, dataset_name, tweets_per_day, \n",
    "                       start_date, end_date, training_partition, show):\n",
    "    # part 1: get tweets and store them\n",
    "    for keyword in keywords:\n",
    "        run(tweets_per_day, keyword, start_date, end_date, show=show)\n",
    "        time.sleep(5) # to avoid IP address being blocked\n",
    "        \n",
    "    # part 2: sentiment analysis and pairing with stock data (making dataset)\n",
    "    make_entire_dataset(keywords, stock_names, dataset_name)\n",
    "    \n",
    "    # part 3: results!\n",
    "    all_classifiers(dataset_name, training_partition)\n",
    "    aggregated(dataset_name, training_partition)\n",
    "    \n",
    "# Change these variables to whatever companies you like\n",
    "# This takes about 2 minutes to run, can see entire process\n",
    "keywords = ['netflix', 'facebook'] # what to search for in tweets\n",
    "stock_names = ['NFLX', 'FB'] # must be correct or will fail\n",
    "dataset_name = 'test02' # what to save dataset as in dataset folder\n",
    "tweets_per_day = 10 # how many tweets to get per day\n",
    "start_date = '2015-01-01' # day to start mining tweets\n",
    "end_date = '2015-01-15' # day to end mining tweets\n",
    "training_partition = .7 # percent of data to be used for training\n",
    "show = True # print each day's progess while scraping, change to False to not do so\n",
    "    \n",
    "# IF YOU RESCRAPE A COMPANY TWICE, delete it's .db file. This won't run until you do \n",
    "# that as a saftey percaution so we wouldn't lost all of our data by accident\n",
    "make_small_version(keywords, stock_names, dataset_name, tweets_per_day, \n",
    "                   start_date, end_date, training_partition, show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
