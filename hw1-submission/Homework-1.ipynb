{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will look at some posts on Stack Overflow during the year of 2015 and measure the similarity of users by looking at the types of questions they answer. We will also analyze the creation dates of questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working on the notebook, let's make sure that everything is setup properly. You should have downloaded and installed\n",
    "* [Anaconda](https://store.continuum.io/cshop/anaconda/)\n",
    "* [Git](http://git-scm.com/downloads)\n",
    "\n",
    "If you are working from the undergraduate lab (on a linux machine) these are both installed, but you need to follow the instructions [from here](https://github.com/datascience16/lectures/blob/master/Lecture2/Getting-Started.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sample request to retrieve the questions posted on Stack Exchange on the first day of 2015. Documentation of the Stack Exchange API can be found [here](https://api.stackexchange.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "start_time = 1420070400 # 01-01-2015 at 00:00:00\n",
    "end_time   = 1420156800 # 01-02-2015 at 00:00:00\n",
    "\n",
    "response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100\" +\n",
    "                        \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\n",
    "                        \"&order=asc&sort=creation&site=stackoverflow\")\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the Stack Exchange API are in [unix epoch time](https://en.wikipedia.org/wiki/Unix_time). The format for the request string is specified [here](https://api.stackexchange.com/docs/questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to print the response that Stack Exchange returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to read the raw response. Instead, we need to decode the raw response as JSON and use the `json` library to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print json.dumps(response.json(), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily see that the response consists of a list of question items. For each of these items, we get information about its attributes such as its `creation_date`, `answer_count`, `owner`, `title`, etc.\n",
    "\n",
    "Notice that has_more is true. To get more items, we can [request the next page](https://api.stackexchange.com/docs/paging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Parsing the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we practice some of the basic Python tools that we learned in class and the powerful string handling methods that Python offers. Our goal is to be able to pick the interesting parts of the response and transform them in a format that will be useful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's isolate the creation_date in the response. Fill in the rest of the ```print_creation_dates_json()``` function that reads the response and prints the creation dates. Notice that a JSON object is basically a dictionary. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_creation_dates_json(response):\n",
    "    \"\"\"\n",
    "    Prints the creation_date of all the questions in the response.\n",
    "    \n",
    "    Parameters:\n",
    "        response: Response object\n",
    "    \"\"\"\n",
    "    data = dict(response.json())['items']\n",
    "    for entry in data:\n",
    "        print entry['creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that calls the ```print_creation_dates_json()``` function to print out all the creation dates of questions posted on the first day in 2015. Please be aware of Stack Exchange's [rate limit](https://api.stackexchange.com/docs/throttle). **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_creation_dates_json(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we have downloaded the [data dump](http://cs-people.bu.edu/kzhao/teaching/stackoverflow-posts-2015.tar.gz) for Stack Overflow's posts in 2015. Note that this file is 10GB. If you don't have space on your computer, you can download it into `/scratch` on one of the machines in the undergrad lab or you can download it onto a USB. You may also want to work with a subset of this data at first, but your solution should be efficient enough to work with the whole dataset. For example, if you call `read()` on this file, you will get a `MemoryError`.\n",
    "\n",
    "Write a function to parse out the questions posted in 2015. These are posts with `PostTypeId=1`. Make a `pandas DataFrame` with 4 columns: `Id`, `CreationDate`, `OwnerUserId`, and the first tag in `Tags`. Save the `DataFrame` to a file named `question_dataframe.csv`. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import csv\n",
    "\n",
    "def parse_to_dataframe(): # takes a little over 4 minutes to run on my system\n",
    "    data = [['Id', 'CreationDate', 'OwnerUserId', 'Tag']] # header row\n",
    "    \n",
    "    post_id = ''\n",
    "    c_date = ''\n",
    "    user_id = ''\n",
    "    tag = ''\n",
    "    filename = 'stackoverflow-posts-2015.xml'\n",
    "    context = etree.iterparse(filename, events=('end',)) # iterate node by node\n",
    "\n",
    "    for event, elem in context:\n",
    "        try:\n",
    "            if elem.attrib['PostTypeId'] == '1': # is a question post, may not exist\n",
    "                user_id = elem.attrib['OwnerUserId'] # may not exist\n",
    "                tag = elem.attrib['Tags'].split('>')[0][1:] # parse for first tag\n",
    "                post_id = elem.attrib['Id']\n",
    "                c_date = elem.attrib['CreationDate']\n",
    "                data += [[post_id, c_date, user_id, tag]] # add column\n",
    "        except KeyError: # PostTypeId or OwnerUserId do not exist\n",
    "            continue\n",
    "\n",
    "    with open('question_dataframe.csv', 'w') as f: # write question data to csv\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Id             CreationDate  OwnerUserId             Tag\n",
      "0        27727385  2015-01-01T00:00:58.253      3210431             php\n",
      "1        27727388  2015-01-01T00:01:43.673       868779      apache-pig\n",
      "2        27727391  2015-01-01T00:02:32.123      4372672             ios\n",
      "3        27727393  2015-01-01T00:02:57.983      2482149      sql-server\n",
      "4        27727394  2015-01-01T00:03:31.337      4263870             php\n",
      "5        27727396  2015-01-01T00:04:01.407      4409381         android\n",
      "6        27727406  2015-01-01T00:05:03.773       875317              c#\n",
      "7        27727407  2015-01-01T00:05:27.167       821742            java\n",
      "8        27727408  2015-01-01T00:05:34.733      2595033              c#\n",
      "9        27727409  2015-01-01T00:06:17.720      1815395          apache\n",
      "10       27727410  2015-01-01T00:06:41.067       541091           mysql\n",
      "11       27727414  2015-01-01T00:07:28.747      1210038      javascript\n",
      "12       27727418  2015-01-01T00:07:39.243      3674356             php\n",
      "13       27727419  2015-01-01T00:07:46.460       347062            html\n",
      "14       27727424  2015-01-01T00:09:28.247      1254618         collada\n",
      "15       27727427  2015-01-01T00:10:05.340      3412951   ruby-on-rails\n",
      "16       27727429  2015-01-01T00:10:29.963      1743377      javascript\n",
      "17       27727433  2015-01-01T00:11:43.463       169992               c\n",
      "18       27727434  2015-01-01T00:12:02.597      4056620             ios\n",
      "19       27727439  2015-01-01T00:12:55.513      2993567            java\n",
      "20       27727442  2015-01-01T00:13:04.980      3085148      powershell\n",
      "21       27727444  2015-01-01T00:13:32.583      2457761            java\n",
      "22       27727446  2015-01-01T00:13:50.463      4371259         node.js\n",
      "23       27727455  2015-01-01T00:15:40.250      3802790             ios\n",
      "24       27727459  2015-01-01T00:17:11.113      2227834               c\n",
      "25       27727464  2015-01-01T00:18:50.297      4397115      javascript\n",
      "26       27727468  2015-01-01T00:22:16.780      3494499             ios\n",
      "27       27727477  2015-01-01T00:23:14.747      3286192         android\n",
      "28       27727481  2015-01-01T00:23:50.863      2999675             c++\n",
      "29       27727484  2015-01-01T00:24:28.103      4289580             php\n",
      "...           ...                      ...          ...             ...\n",
      "2518823  34552465  2015-12-31T23:41:49.993      5633002             php\n",
      "2518824  34552466  2015-12-31T23:42:01.600      5728911            java\n",
      "2518825  34552470  2015-12-31T23:42:45.380      1763652           mysql\n",
      "2518826  34552473  2015-12-31T23:44:33.663      4673488          python\n",
      "2518827  34552480  2015-12-31T23:45:33.357      5735042            java\n",
      "2518828  34552482  2015-12-31T23:45:48.310       831878               k\n",
      "2518829  34552483  2015-12-31T23:45:48.627       510296             dom\n",
      "2518830  34552486  2015-12-31T23:46:08.790      3035850            xmpp\n",
      "2518831  34552487  2015-12-31T23:46:10.497      5735090            ruby\n",
      "2518832  34552488  2015-12-31T23:46:12.693      5299726         windows\n",
      "2518833  34552492  2015-12-31T23:47:07.157      1108057             php\n",
      "2518834  34552500  2015-12-31T23:48:17.143      5735067             tcp\n",
      "2518835  34552501  2015-12-31T23:48:54.070      4838216           mysql\n",
      "2518836  34552504  2015-12-31T23:49:14.810      2665223       angularjs\n",
      "2518837  34552506  2015-12-31T23:50:00.363        15441            java\n",
      "2518838  34552508  2015-12-31T23:50:53.880       592888             api\n",
      "2518839  34552510  2015-12-31T23:51:22.323      5009786             osx\n",
      "2518840  34552511  2015-12-31T23:51:27.817      5735089             c++\n",
      "2518841  34552513  2015-12-31T23:51:36.167       254046            ruby\n",
      "2518842  34552515  2015-12-31T23:52:14.067       259602             uml\n",
      "2518843  34552519  2015-12-31T23:53:52.380      5618866               c\n",
      "2518844  34552527  2015-12-31T23:54:25.837       973158            html\n",
      "2518845  34552535  2015-12-31T23:56:12.397      2705042          vb.net\n",
      "2518846  34552536  2015-12-31T23:56:16.013      4991888      javascript\n",
      "2518847  34552539  2015-12-31T23:57:03.073       441016  actionscript-3\n",
      "2518848  34552540  2015-12-31T23:57:08.807      5377088           mysql\n",
      "2518849  34552543  2015-12-31T23:57:36.433      1040740            java\n",
      "2518850  34552547  2015-12-31T23:58:06.150      3909898             ios\n",
      "2518851  34552549  2015-12-31T23:59:30.960      1870790             c++\n",
      "2518852  34552827  2015-12-31T06:02:40.277      2340452           flask\n",
      "\n",
      "[2518853 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "question_df = pandas.read_csv('question_dataframe.csv')\n",
    "print question_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tackle our original problem. Write a function to measure the similarity of the top 1000 users with the most answer posts. Compare the users based on the types of questions they answer. We will categorize the questions by looking at the first tag in each question. You may choose to implement any one of the similarity/distance measures we discussed in class. Document your findings. **(30pts)**\n",
    "\n",
    "Note that answers are posts with `PostTypeId=2`. The ID of the question in answer posts is the `ParentId`.\n",
    "\n",
    "You may find the [sklearn.feature_extraction module](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from lxml import etree\n",
    "\n",
    "def measure_similarity(df=question_df): # takes about 7 minutes to run on my system\n",
    "    \n",
    "    parent_id = ''\n",
    "    user_id = ''\n",
    "    \n",
    "    user_answers = dict() # dictionary of dictionaries to keep track of tags by user\n",
    "    filename = 'stackoverflow-posts-2015.xml'\n",
    "    context = etree.iterparse(filename, events=('end',)) # iterate node by node\n",
    "\n",
    "    for event, elem in context:\n",
    "        try:\n",
    "            if elem.attrib['PostTypeId'] == '2': # answer_post\n",
    "                user_id = elem.attrib['OwnerUserId'] # may not exist, need try block\n",
    "                parent_id = int(elem.attrib['ParentId'])\n",
    "                if user_id in user_answers:\n",
    "                    user_answers[user_id] += [parent_id] # add parent id\n",
    "                else:\n",
    "                    user_answers[user_id] = [parent_id] # add new list with parent id\n",
    "        except: # PostTypeId or OwnerUserId do not exist, or question was asked before 2015\n",
    "            continue\n",
    "\n",
    "    # clean up user_answers and sort to top 1000\n",
    "    user_answers = user_answers.items()\n",
    "    user_answers.sort(key=lambda x:len(x[1]), reverse=True) # sort by length value, not key\n",
    "    user_answers = user_answers[:1000] # get top 1000 users\n",
    "    \n",
    "    # preprocess tags to dictionary for significant lookup time reduction\n",
    "    tags = dict()\n",
    "    for i in range(len(df['Tag'])):\n",
    "        tags[df['Id'][i]] = df['Tag'][i]\n",
    "    \n",
    "    # create list of dictionaries of tags for each user\n",
    "    user_list = ['' for i in range(len(user_answers))]\n",
    "    tag_list = [{} for i in range(len(user_answers))]\n",
    "    for i in range(len(user_answers)):\n",
    "        user_list[i] = user_answers[i][0] # keep track of users\n",
    "        for parent_id in user_answers[i][1]:\n",
    "            try: # in case question asked before 2015 and there is no tag\n",
    "                tag = tags[parent_id] # grab the tag\n",
    "                tag_list[i][tag] = tag_list[i].get(tag, 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # convert to integer matrix for analysis\n",
    "    vec = DictVectorizer()\n",
    "    X = vec.fit_transform(tag_list).toarray() # integer vectors of tag count per user\n",
    "    y = vec.get_feature_names() # tag label of each index in vector\n",
    "    \n",
    "    # get top tags of top 10 users and last 10 users\n",
    "    tags_top = set()\n",
    "    tags_bottom = set()\n",
    "    for i in range(10):\n",
    "        tags_top = tags_top.union({y[list(X[i]).index(max(X[i]))]})\n",
    "    for i in range(len(X)-10, len(X)):\n",
    "        tags_bottom = tags_bottom.union({y[list(X[i]).index(max(X[i]))]})\n",
    "        \n",
    "    print tags_top\n",
    "    print tags_bottom\n",
    "    \n",
    "    return user_list, X, y\n",
    "\n",
    "user_list, X, y = measure_similarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a subset of the distance matrix. Order the pairwise distance in your distance matrix (excluding the entries along the diagonal) in increasing order and pick user pairs until you have 100 unique users. See [Lecture 3](https://github.com/datascience16/lectures/blob/master/Lecture3/Distance-Functions.ipynb) for examples. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def matrix_subset(X=euclidean_dists):\n",
    "    one_d = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(i):\n",
    "            one_d += [(X[i][j], i, j)]\n",
    "    one_d = sorted(one_d)\n",
    "    unique = []\n",
    "    d, i, j = None, None, None\n",
    "    n = 0\n",
    "    while len(unique) < 100:\n",
    "        d, i, j = one_d[n]\n",
    "        n += 1\n",
    "        if i in unique or j in unique:\n",
    "            continue\n",
    "        else:\n",
    "            unique += [i, j]\n",
    "    new_matrix = [[0]*100 for i in range(100)]\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            new_matrix[i][j] = X[unique[j]][unique[i]]\n",
    "    return new_matrix\n",
    "\n",
    "euclidean_dists = metrics.euclidean_distances(X)\n",
    "subsets = matrix_subset()\n",
    "sns.heatmap(subsets, xticklabels=False, yticklabels=False, linewidths=0, ax=ax2, square=True,cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create some time series from the data. Look at the top 100 users with the most question posts. For each user, your time series will be the `CreationDate` of the questions posted by that user. You may want to make multiple time series for each user based on the first tag of the questions. Compare the time series using one of the methods discussed in class. Document your findings. **(30 pts)**\n",
    "\n",
    "You may find the [pandas.DataFrame.resample module](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b7f3831fd3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0muser_y_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m365\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0muser_y_axis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# increment index of date, represents answer on that day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0muser_ids\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# add user_id to final list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def dtw_dist(list1, list2): # dynamic time warping distance between 2 lists of y values\n",
    "    d = {}\n",
    "    for i in range(len(list1)):\n",
    "        d[(i, -1)] = float('inf')\n",
    "    for i in range(len(list2)):\n",
    "        d[(-1, i)] = float('inf')\n",
    "    d[(-1, -1)] = 0\n",
    "    \n",
    "    distance = 0\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list2)):\n",
    "            distance = (list1[i] - list2[i])**2\n",
    "            d[(i, j)] = distance + min(d[(i-1, j)], d[(i, j-1)], d[(i-1, j-1)])  \n",
    "    return pow(d[(len(list1)-1, len(list2)-1)], .5)\n",
    "    \n",
    "user_questions = dict() # key: user_id, value: list of dates of question posts\n",
    "\n",
    "user_id = ''\n",
    "date = None\n",
    "\n",
    "# fill in user_questions with creation dates\n",
    "for i in range(len(question_df['Tag'])):\n",
    "    user_id = question_df['OwnerUserId'][i]\n",
    "    date = datetime.strptime(question_df['CreationDate'][i][:10], '%Y-%m-%d') # parse to python datetime object\n",
    "    if user_id in user_questions:\n",
    "        user_questions[user_id] += [date]\n",
    "    else:\n",
    "        user_questions[user_id] = [date]\n",
    "    \n",
    "# clean up user_questions and sort to top 100\n",
    "user_questions = user_questions.items()\n",
    "user_questions.sort(key=lambda x:len(x[1]), reverse=True) # sort by length value, not key\n",
    "user_questions = user_questions[:100] # get top 100 users\n",
    "\n",
    "# create 365 day long timeseries index dictionary (for faster processing time later)\n",
    "date_index = dict()\n",
    "base = datetime.strptime('2015-01-01', '%Y-%m-%d')\n",
    "all_2015 = [base + timedelta(days=i) for i in range(365)]\n",
    "for i in range(365):\n",
    "    date_index[all_2015[i]] = i\n",
    "    \n",
    "# make y axis of time series for each user\n",
    "user_ids = [] # final list for user_ids\n",
    "user_timeseries = [] # final list for user 365-day timeseries\n",
    "\n",
    "user_y_axis = []\n",
    "for i in range(100):\n",
    "    user_y_axis = [0 for i in range(365)]\n",
    "    for date in user_questions[i][1]:\n",
    "        user_y_axis[date_index[date]] += 1 # increment index of date, represents answer on that day\n",
    "    user_ids += [user_questions[i][0]] # add user_id to final list\n",
    "    user_timeseries += [user_y_axis] # add time series to final list\n",
    "    \n",
    "# make distance matrix\n",
    "matrix = [[0]*100 for i in range(100)]\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if i == j: # are the same timeseries\n",
    "            continue\n",
    "        else:\n",
    "            if matrix[j][i] == 0: # not computed yet\n",
    "                matrix[i][j] = dtw_dist(user_timeseries[i], user_timeseries[j])\n",
    "            else: # computed on other half of matrix, don't redo it\n",
    "                matrix[i][j] = matrix[j][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2 most similar and the 2 most different time series. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "    \n",
    "def overlay(y1, y2, t, all_2015=all_2015):\n",
    "    plt.plot(all_2015, y1, color='blue')\n",
    "    plt.plot(all_2015, y2, color='red')\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "minv = float('inf')\n",
    "maxv = float('-inf')\n",
    "max_2 = [0, 0]\n",
    "min_2 = [0, 0]\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            if matrix[i][j] > maxv:\n",
    "                max_2 = [i, j]\n",
    "                maxv = matrix[i][j]\n",
    "            if matrix[i][j] < minv:\n",
    "                min_2 = [i, j]\n",
    "                minv = matrix[i][j]\n",
    "\n",
    "overlay(user_timeseries[min_2[0]], user_timeseries[min_2[1]], 'Two Most Similiar Users')\n",
    "overlay(user_timeseries[max_2[0]], user_timeseries[max_2[1]], 'Two Most Different Users')\n",
    "\n",
    "sns.heatmap(matrix, xticklabels=False, yticklabels=False, linewidths=0, square=True,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"..themes/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
